








  2%|▏         | 10/475 [00:17<13:26,  1.73s/it]









  4%|▍         | 20/475 [00:35<13:26,  1.77s/it]









  6%|▋         | 30/475 [00:53<13:15,  1.79s/it]









  8%|▊         | 40/475 [01:12<13:09,  1.82s/it]








 11%|█         | 50/475 [01:30<13:06,  1.85s/it][INFO|trainer.py:3376] 2024-02-29 02:39:18,661 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-02-29 02:39:18,661 >>   Num examples = 100
[INFO|trainer.py:3381] 2024-02-29 02:39:18,661 >>   Batch size = 8
{'loss': 1.3039, 'grad_norm': 23.90106201171875, 'learning_rate': 9.77037682640015e-07, 'epoch': 0.11}







 85%|████████▍ | 11/13 [00:12<00:02,  1.28s/it]








 12%|█▏        | 59/475 [02:02<15:06,  2.18s/it]









 15%|█▍        | 69/475 [02:21<12:07,  1.79s/it]










 17%|█▋        | 80/475 [02:41<12:07,  1.84s/it]









 19%|█▉        | 90/475 [02:59<11:44,  1.83s/it]








 21%|██        | 100/475 [03:18<11:29,  1.84s/it][INFO|trainer.py:3376] 2024-02-29 02:41:06,692 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-02-29 02:41:06,692 >>   Num examples = 100
[INFO|trainer.py:3381] 2024-02-29 02:41:06,692 >>   Batch size = 8
{'loss': 1.1599, 'grad_norm': 25.250211715698242, 'learning_rate': 9.045084971874737e-07, 'epoch': 0.21}







 85%|████████▍ | 11/13 [00:12<00:02,  1.29s/it]









 23%|██▎       | 110/475 [03:51<12:03,  1.98s/it]









 25%|██▌       | 120/475 [04:10<10:28,  1.77s/it]









 27%|██▋       | 130/475 [04:28<10:18,  1.79s/it]








 29%|██▉       | 139/475 [04:44<09:42,  1.73s/it]









 31%|███▏      | 149/475 [05:02<09:57,  1.83s/it]
 32%|███▏      | 150/475 [05:03<09:55,  1.83s/it][INFO|trainer.py:3376] 2024-02-29 02:42:52,302 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-02-29 02:42:52,302 >>   Num examples = 100
[INFO|trainer.py:3381] 2024-02-29 02:42:52,302 >>   Batch size = 8







 85%|████████▍ | 11/13 [00:13<00:02,  1.29s/it]









 34%|███▎      | 160/475 [05:38<10:55,  2.08s/it]









 36%|███▌      | 170/475 [05:56<09:14,  1.82s/it]









 38%|███▊      | 179/475 [06:13<09:07,  1.85s/it]









 40%|███▉      | 189/475 [06:32<08:53,  1.87s/it]









 42%|████▏     | 199/475 [06:49<07:59,  1.74s/it]
 42%|████▏     | 200/475 [06:51<07:55,  1.73s/it][INFO|trainer.py:3376] 2024-02-29 02:44:39,839 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-02-29 02:44:39,839 >>   Num examples = 100
[INFO|trainer.py:3381] 2024-02-29 02:44:39,839 >>   Batch size = 8







 92%|█████████▏| 12/13 [00:14<00:01,  1.28s/it]









 44%|████▍     | 209/475 [07:23<08:54,  2.01s/it]









 46%|████▋     | 220/475 [07:43<07:44,  1.82s/it]









 48%|████▊     | 230/475 [08:00<07:17,  1.79s/it]









 50%|█████     | 239/475 [08:17<07:20,  1.87s/it]









 52%|█████▏    | 249/475 [08:35<06:38,  1.76s/it]
 53%|█████▎    | 250/475 [08:37<06:49,  1.82s/it][INFO|trainer.py:3376] 2024-02-29 02:46:26,195 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-02-29 02:46:26,196 >>   Num examples = 100
[INFO|trainer.py:3381] 2024-02-29 02:46:26,196 >>   Batch size = 8







 92%|█████████▏| 12/13 [00:14<00:01,  1.28s/it]









 55%|█████▍    | 259/475 [09:10<07:41,  2.14s/it]










 57%|█████▋    | 270/475 [09:30<06:25,  1.88s/it]









 59%|█████▉    | 280/475 [09:48<05:58,  1.84s/it]









 61%|██████    | 290/475 [10:06<05:44,  1.86s/it]









 63%|██████▎   | 299/475 [10:23<05:24,  1.84s/it]
 63%|██████▎   | 300/475 [10:25<05:33,  1.90s/it][INFO|trainer.py:3376] 2024-02-29 02:48:14,189 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-02-29 02:48:14,190 >>   Num examples = 100
[INFO|trainer.py:3381] 2024-02-29 02:48:14,190 >>   Batch size = 8







 92%|█████████▏| 12/13 [00:14<00:01,  1.28s/it]









 65%|██████▌   | 310/475 [10:59<05:22,  1.96s/it]









 67%|██████▋   | 319/475 [11:16<04:42,  1.81s/it]









 69%|██████▉   | 329/475 [11:34<04:28,  1.84s/it]









 71%|███████▏  | 339/475 [11:52<04:08,  1.83s/it]









 74%|███████▎  | 350/475 [12:12<03:59,  1.91s/it][INFO|trainer.py:3376] 2024-02-29 02:50:00,942 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-02-29 02:50:00,942 >>   Num examples = 100
[INFO|trainer.py:3381] 2024-02-29 02:50:00,942 >>   Batch size = 8
  0%|          | 0/13 [00:00<?, ?it/s]








 85%|████████▍ | 11/13 [00:13<00:02,  1.30s/it]









 76%|███████▌  | 360/475 [12:46<03:43,  1.94s/it]









 78%|███████▊  | 370/475 [13:04<03:14,  1.85s/it]









 80%|████████  | 380/475 [13:23<03:00,  1.90s/it]









 82%|████████▏ | 390/475 [13:41<02:36,  1.84s/it]









 84%|████████▍ | 399/475 [13:58<02:23,  1.89s/it]
 84%|████████▍ | 400/475 [14:00<02:23,  1.92s/it][INFO|trainer.py:3376] 2024-02-29 02:51:48,861 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-02-29 02:51:48,861 >>   Num examples = 100
[INFO|trainer.py:3381] 2024-02-29 02:51:48,861 >>   Batch size = 8







 92%|█████████▏| 12/13 [00:14<00:01,  1.28s/it]









 86%|████████▋ | 410/475 [14:35<02:14,  2.06s/it]









 88%|████████▊ | 420/475 [14:53<01:38,  1.79s/it]









 91%|█████████ | 430/475 [15:11<01:23,  1.86s/it]









 93%|█████████▎| 440/475 [15:29<01:03,  1.81s/it]








 95%|█████████▍| 450/475 [15:47<00:43,  1.75s/it][INFO|trainer.py:3376] 2024-02-29 02:53:35,414 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-02-29 02:53:35,414 >>   Num examples = 100
[INFO|trainer.py:3381] 2024-02-29 02:53:35,414 >>   Batch size = 8
{'loss': 1.0205, 'grad_norm': 23.899423599243164, 'learning_rate': 9.810017062595321e-09, 'epoch': 0.95}







 85%|████████▍ | 11/13 [00:13<00:02,  1.30s/it]









 97%|█████████▋| 460/475 [16:21<00:31,  2.08s/it]









 99%|█████████▉| 470/475 [16:39<00:08,  1.78s/it]




100%|█████████▉| 474/475 [16:46<00:01,  1.90s/it]
{'train_runtime': 1013.6021, 'train_samples_per_second': 1.875, 'train_steps_per_second': 0.469, 'train_loss': 1.0691215836374384, 'epoch': 1.0}
02/29/2024 02:54:36 - INFO - llmtuner.extras.misc - Value head model saved at: ./rm_checkpoint/gemma/test
***** train metrics *****
  epoch                    =        1.0
  train_loss               =     1.0691
  train_runtime            = 0:16:53.60
  train_samples_per_second =      1.875
  train_steps_per_second   =      0.469
Figure saved: ./rm_checkpoint/gemma/test/training_loss.png
100%|██████████| 475/475 [16:48<00:00,  1.83s/it][INFO|trainer.py:2067] 2024-02-29 02:54:36,879 >>
Training completed. Do not forget to share your model on huggingface.co/models =)
100%|██████████| 475/475 [16:48<00:00,  2.12s/it]
[INFO|trainer.py:3067] 2024-02-29 02:54:36,880 >> Saving model checkpoint to ./rm_checkpoint/gemma/test
[INFO|trainer.py:3081] 2024-02-29 02:54:36,881 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|tokenization_utils_base.py:2459] 2024-02-29 02:54:36,882 >> tokenizer config file saved in ./rm_checkpoint/gemma/test/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-02-29 02:54:36,882 >> Special tokens file saved in ./rm_checkpoint/gemma/test/special_tokens_map.json
/home/student2021/anaconda3/envs/llama-factory/lib/python3.10/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in /home/student2021/srcao/base_LLM/gemma-7b - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-02-29 02:54:37,162 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-02-29 02:54:37,162 >>   Num examples = 100
[INFO|trainer.py:3381] 2024-02-29 02:54:37,162 >>   Batch size = 8







 92%|█████████▏| 12/13 [00:14<00:01,  1.28s/it]
***** eval metrics *****
  epoch                   =        1.0
  eval_accuracy           =       0.45
  eval_loss               =      0.921
  eval_runtime            = 0:00:15.96
  eval_samples_per_second =      6.265
100%|██████████| 13/13 [00:14<00:00,  1.14s/it]
[INFO|modelcard.py:450] 2024-02-29 02:54:53,127 >> Dropping the following result as it does not have all the necessary fields:
{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}, 'metrics': [{'name': 'Accuracy', 'type': 'accuracy', 'value': 0.45}]}